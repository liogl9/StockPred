{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Setup complete. Using torch {torch.__version__}({device})\")\n",
    "torch.manual_seed(42)\n",
    "lookback = 30\n",
    "PATH = os.path.join(os.getcwd(),\"models\",\"{}daysLoopback\".format(lookback),\"btc5y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc = yf.Ticker(\"BTC-USD\")\n",
    "historico_btc = btc.history(period=\"5y\")\n",
    "print(historico_btc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = historico_btc.iloc[:,1:2].values.astype('float32')\n",
    "stock_price = stock_price.reshape((-1,1))\n",
    "\n",
    "plt.plot(stock_price)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "full_min = stock_price.min()\n",
    "full_max = stock_price.max()\n",
    "sc_full = MinMaxScaler(feature_range=(full_min, full_max))\n",
    "training_set_scaled = sc.fit_transform(stock_price)\n",
    "# training_set_scaled = stock_price\n",
    "print(\"Min: \",stock_price.min())\n",
    "print(\"Max: \", stock_price.max())\n",
    "plt.plot(training_set_scaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-lookback):\n",
    "        feature = dataset[i:i+lookback]\n",
    "        target = dataset[i+lookback:i+lookback+1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    \n",
    "    return torch.tensor(np.array(X)), torch.tensor(np.array(y))\n",
    "def create_single_sample(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    feature = dataset[0:lookback]\n",
    "    target = dataset[1:lookback]\n",
    "    X.append(feature)\n",
    "    y.append(target)\n",
    "    return torch.tensor(np.array(X)), torch.tensor(np.array(y))\n",
    "def get_single_sample(dataset, lookback, pos):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    feature = dataset[pos:pos+lookback]\n",
    "    target = dataset[pos+1:pos+lookback+1]\n",
    "    X.append(feature)\n",
    "    y.append(target)\n",
    "        \n",
    "    return torch.tensor(np.array(X)), torch.tensor(np.array(y))\n",
    "# train-val split for time series\n",
    "train_size = int(len(training_set_scaled) * 0.67)\n",
    "val_size = len(training_set_scaled) - train_size\n",
    "train, val = training_set_scaled[:train_size], training_set_scaled[train_size:]\n",
    "X_train, y_train = create_dataset(train, lookback=lookback)\n",
    "X_val, y_val = create_dataset(val, lookback=lookback)\n",
    "X_full, y_full = create_single_sample(training_set_scaled, lookback=lookback)\n",
    "train_min = stock_price[:train_size,:].min()\n",
    "train_max = stock_price[:train_size,:].max()\n",
    "val_min = stock_price[train_size:,:].min()\n",
    "val_max = stock_price[train_size:,:].max()\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(\"train Min: \",stock_price[:train_size,:].min())\n",
    "print(\"val Min: \",stock_price[train_size:,:].min())\n",
    "print(\"train max: \",stock_price[:train_size,:].max())\n",
    "print(\"val max: \",stock_price[train_size:,:].max())\n",
    "sc_train = MinMaxScaler(feature_range=(train_min, train_max))\n",
    "sc_val = MinMaxScaler(feature_range=(val_min, val_max))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMOnlyLast(nn.Module):\n",
    "    def __init__(self,hidden_size=50, num_layers=1, dropout = 0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        if num_layers ==1:\n",
    "            dropout=0\n",
    "        self.lstm_0 = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=num_layers, batch_first=True,dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h_0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        x, _ = self.lstm_0(x,(h_0,c_0))\n",
    "        x = x[:, -1, :]\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "class LSTMWhole(nn.Module):\n",
    "    def __init__(self,hidden_size=50, num_layers=1, dropout = 0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        if num_layers ==1:\n",
    "            dropout=0\n",
    "        self.lstm_0 = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=num_layers, batch_first=True,dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h_0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        x, _ = self.lstm_0(x,(h_0,c_0))\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMOnlyLast(20,1,0).to(device)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=0.0005)\n",
    "loss_fn = nn.MSELoss()\n",
    "train_loader = data.DataLoader(data.TensorDataset(X_train, y_train), shuffle=False, batch_size=8)\n",
    "val_loader = data.DataLoader(data.TensorDataset(X_val, y_val), shuffle=False, batch_size=8)\n",
    "n_epochs = 2000\n",
    "best_loss = 100\n",
    "y_full_preds = np.ones((training_set_scaled.shape[0]-lookback,1))*np.nan\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(torch.unsqueeze(y_pred, dim=-1), y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    if epoch % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for pred in range(training_set_scaled.shape[0]-lookback):\n",
    "                X_full_gpu = X_full.to(device)\n",
    "                y_pred_full = model(X_full_gpu)\n",
    "                y_pred_noise = (y_pred_full + X_full_gpu[:,-1,:])/2\n",
    "                # y_pred_noise = y_pred_full\n",
    "                # y_full_preds[pred,:] = torch.unsqueeze(y_pred_full.cpu(), dim=-1)[:, -1, :]\n",
    "                y_full_preds[pred,:] = torch.unsqueeze(y_pred_noise.cpu(), dim=-1)[:, -1, :]\n",
    "                new_input = np.ones_like(X_full.numpy())\n",
    "                new_input[:,:-1,:] = X_full.numpy()[:,1:,:]\n",
    "                # new_input[:,-1,:] = torch.unsqueeze(y_pred_full.cpu(), dim=-1)[:, -1, :]\n",
    "                new_input[:,-1,:] = torch.unsqueeze(y_pred_noise.cpu(), dim=-1)[:, -1, :]\n",
    "                X_full = torch.tensor(new_input)\n",
    "                full_test_plot = np.ones((training_set_scaled.shape[0],1)) * np.nan\n",
    "                full_test_plot[lookback:] = sc_full.fit_transform(y_full_preds.reshape(-1,1))\n",
    "                # full_test_plot[lookback:] = y_full_preds.reshape(-1,1)\n",
    "                if pred % 5 == 0:\n",
    "                    X_samp, y_samp = get_single_sample(training_set_scaled, lookback, pred)\n",
    "                    # X_full = (X_full + X_samp)/2\n",
    "                    X_full = (X_samp)\n",
    "\n",
    "            X_full, y_full = create_single_sample(train, lookback=lookback)\n",
    "            X_train_gpu = X_train.to(device)\n",
    "            X_val_gpu = X_val.to(device)\n",
    "            y_pred_train = model(X_train_gpu)\n",
    "            train_rmse = np.sqrt(loss_fn(torch.unsqueeze(y_pred_train.cpu(), dim=-1), y_train))\n",
    "            y_pred_val = model(X_val_gpu)\n",
    "            val_rmse = np.sqrt(loss_fn(torch.unsqueeze(y_pred_val.cpu(), dim=-1), y_val))\n",
    "            # shift train predictions for plotting\n",
    "            train_plot = np.ones_like(training_set_scaled) * np.nan\n",
    "            # train_plot[lookback:train_size] = sc_train.fit_transform(y_pred_train.cpu()[:, -1, :])\n",
    "            # train_plot[lookback:train_size] = y_pred_train.cpu()[:, -1, :]\n",
    "            train_plot[lookback:train_size] = sc_train.fit_transform(torch.unsqueeze(y_pred_train.cpu(), dim=-1)[:, -1, :])\n",
    "            # train_plot[lookback:train_size] = torch.unsqueeze(y_pred_train.cpu(), dim=-1)[:, -1, :]\n",
    "            # shift val predictions for plotting\n",
    "            val_plot = np.ones_like(training_set_scaled) * np.nan\n",
    "            # val_plot[train_size+lookback:len(training_set_scaled)] = sc_val.fit_transform(y_pred_val.cpu()[:, -1, :])\n",
    "            val_plot[train_size+lookback:len(training_set_scaled)] = sc_val.fit_transform(torch.unsqueeze(y_pred_val.cpu(), dim=-1)[:, -1, :])\n",
    "            # val_plot[train_size+lookback:len(training_set_scaled)] = torch.unsqueeze(y_pred_val.cpu(), dim=-1)[:, -1, :]\n",
    "            # plot\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(stock_price, c='b')\n",
    "            ax.plot(train_plot, c='r')\n",
    "            ax.plot(val_plot, c='g')\n",
    "            ax.plot(full_test_plot,c='m')\n",
    "            start, end = ax.get_xlim()\n",
    "            ax.xaxis.set_ticks(np.arange(0, end, 30))\n",
    "            ax.grid()\n",
    "            plt.show()\n",
    "            if val_rmse < best_loss:\n",
    "                best_loss = val_rmse\n",
    "                try:\n",
    "                    torch.save(model.state_dict(), os.path.join(PATH,\"best_model.pt\"))\n",
    "                except:\n",
    "                    os.makedirs(PATH)\n",
    "                    torch.save(model.state_dict(), os.path.join(PATH,\"best_model.pt\"))\n",
    "\n",
    "                print(\"New Best model saved\")\n",
    "\n",
    "\n",
    "        print(\"Epoch %d: train RMSE %.4f, val RMSE %.4f, best val %.4f\" % (epoch, train_rmse, val_rmse, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(stock_price, c='b')\n",
    "ax.plot(train_plot, c='r')\n",
    "ax.plot(val_plot, c='g')\n",
    "# ax.plot(full_test_plot,c='m')\n",
    "start, end = ax.get_xlim()\n",
    "ax.xaxis.set_ticks(np.arange(0, end, 1))\n",
    "ax.grid()\n",
    "plt.xlim(1770,1800)\n",
    "plt.ylim(20000,40000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PredicciÃ³n de valores futuros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc = yf.Ticker(\"BTC-USD\")\n",
    "historico_btc = btc.history(period=\"5y\")\n",
    "stock_price = historico_btc.iloc[:,1:2].values.astype('float32')\n",
    "stock_price = stock_price.reshape((-1,1))\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "test_set_scaled = sc.fit_transform(stock_price)\n",
    "\n",
    "\n",
    "def create_dataset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-lookback):\n",
    "        feature = dataset[i:i+lookback]\n",
    "        target = dataset[i+1:i+lookback+1]\n",
    "        X.append(feature)\n",
    "        y.append(target)\n",
    "    return torch.tensor(np.array(X)), torch.tensor(np.array(y))\n",
    "\n",
    "def create_single_sample(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    feature = dataset[0:lookback]\n",
    "    target = dataset[1:lookback+1]\n",
    "    X.append(feature)\n",
    "    y.append(target)\n",
    "    return torch.tensor(np.array(X)), torch.tensor(np.array(y))\n",
    "    \n",
    "\n",
    "def create_testset(dataset, lookback):\n",
    "    \"\"\"Transform a time series into a prediction dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: A numpy array of time series, first dimension is the time steps\n",
    "        lookback: Size of window for prediction\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    for i in range(len(dataset)-lookback):\n",
    "        feature = dataset[i:i+lookback]\n",
    "        X.append(feature)\n",
    "    \n",
    "    return torch.tensor(np.array(X))\n",
    "\n",
    "train_size = int(len(test_set_scaled) * 0.67)\n",
    "val_size = len(test_set_scaled) - train_size\n",
    "train, val = test_set_scaled[:train_size], test_set_scaled[train_size:]\n",
    "lookback = 30\n",
    "X_val, y_val = create_dataset(val, lookback=lookback)\n",
    "X_val_test, y_val_test = create_single_sample(val, lookback=lookback)\n",
    "X_full, y_full = create_single_sample(train, lookback=lookback)\n",
    "val_min = stock_price[train_size:,:].min()\n",
    "val_max = stock_price[train_size:,:].max()\n",
    "sc_val = MinMaxScaler(feature_range=(val_min, val_max))\n",
    "test = test_set_scaled[-lookback-1:]\n",
    "X_test = create_testset(test, lookback=lookback)\n",
    "print(X_test.shape)\n",
    "test_min = stock_price[-lookback:,:].min()\n",
    "test_max = stock_price[-lookback:,:].max()\n",
    "sc_test = MinMaxScaler(feature_range=(test_min, test_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMOnlyLast(20,1,0).to(device)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 300\n",
    "model_path = PATH = os.path.join(os.getcwd(),\"models\",\"{}daysLoopback\".format(lookback),\"btc5y\", \"btc_pred_500_epoch.pt\")\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "n_preds = 31\n",
    "y_preds = np.ones((n_preds,1))*np.nan\n",
    "y_val_preds = np.ones((val_size-lookback,1))*np.nan\n",
    "y_full_preds = np.ones((test_set_scaled.shape[0]-lookback,1))*np.nan\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_val_gpu = X_val.to(device)\n",
    "    y_pred_val = model(X_val_gpu)\n",
    "    val_plot = np.ones_like(test_set_scaled) * np.nan\n",
    "    val_plot[train_size+lookback:len(test_set_scaled)] = y_pred_val.cpu()[:, -1, :]\n",
    "    for pred in range(test_set_scaled.shape[0]-lookback):\n",
    "        X_full_gpu = X_full.to(device)\n",
    "        y_pred_full = model(X_full_gpu)\n",
    "        y_full_preds[pred,:] = y_pred_full.cpu()[:, -1, :]\n",
    "        new_input = np.ones_like(X_full.numpy())\n",
    "        new_input[:,:-1,:] = X_full.numpy()[:,1:,:]\n",
    "        new_input[:,-1,:] = y_pred_full.cpu()[:, -1, :]\n",
    "        X_full = torch.tensor(new_input)\n",
    "        full_test_plot = np.ones((test_set_scaled.shape[0],1)) * np.nan\n",
    "        full_test_plot[lookback:] = y_full_preds.reshape(-1,1)\n",
    "    for val_pred in range(val_size-lookback):\n",
    "        X_val_test_gpu = X_val_test.to(device)\n",
    "        y_pred_val_test = model(X_val_test_gpu)\n",
    "        y_val_preds[val_pred,:] = y_pred_val_test.cpu()[:, -1, :]\n",
    "        new_input = np.ones_like(X_val_test.numpy())\n",
    "        new_input[:,:-1,:] = X_val_test.numpy()[:,1:,:]\n",
    "        new_input[:,-1,:] = y_pred_val_test.cpu()[:, -1, :]\n",
    "        X_val_test = torch.tensor(new_input)\n",
    "        val_test_plot = np.ones((test_set_scaled.shape[0],1)) * np.nan\n",
    "        val_test_plot[train_size+lookback:] = y_val_preds.reshape(-1,1)\n",
    "    for n_pred in range(n_preds):\n",
    "        X_test_gpu = X_test.to(device)\n",
    "        y_pred_test = model(X_test_gpu)\n",
    "        y_preds[n_pred,:] = y_pred_test.cpu()[:, -1, :]\n",
    "        new_input = np.ones_like(X_test.numpy())\n",
    "        new_input[:,:-1,:] = X_test.numpy()[:,1:,:]\n",
    "        new_input[:,-1,:] = y_pred_test.cpu()[:, -1, :]\n",
    "        X_test = torch.tensor(new_input)\n",
    "        test_plot = np.ones((test_set_scaled.shape[0]+n_preds,1)) * np.nan\n",
    "        test_plot[len(test_set_scaled):len(test_set_scaled)+n_preds] = y_preds.reshape(-1,1)\n",
    "        # plot\n",
    "plt.plot(test_set_scaled, c='b')\n",
    "plt.plot(val_plot,c='g')\n",
    "plt.plot(val_test_plot,c='r')\n",
    "plt.plot(test_plot, c='g')\n",
    "plt.plot(full_test_plot,c='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set_scaled, c='b')\n",
    "plt.plot(val_plot,c='g')\n",
    "plt.plot(val_test_plot,c='r')\n",
    "plt.plot(test_plot, c='g')\n",
    "plt.xlim((500,600))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stockpred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
